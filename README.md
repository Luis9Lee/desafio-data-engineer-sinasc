# desafio-data-engineer-sinasc
Solu√ß√£o de Engenharia de Dados para construir a funda√ß√£o de um Data Lakehouse, processando dados brutos do SINASC e criando um modelo Star Schema para an√°lise de sa√∫de materno-infantil na Baixada Santista, SP.

# üè• Data Lakehouse para An√°lise de Sa√∫de Materno-Infantil - SP

## üåü Vis√£o Geral

Este projeto implementa um Data Lakehouse completo para monitoramento de sa√∫de materno-infantil no Estado de S√£o Paulo, utilizando dados dos sistemas SINASC (nascimentos) e SIM-DOINF (√≥bitos infantis) do DATASUS.

**Per√≠odo dos dados:** 2010 a 2024  
**Ferramentas:** Databricks, PySpark, Spark SQL, Delta Lake  
**Arquitetura:** Medalh√£o (Bronze ‚Üí Silver ‚Üí Gold) com Star Schema na camada final

## üèóÔ∏è Arquitetura do Projeto

### Arquitetura Medalh√£o Implementada

A arquitetura segue o padr√£o Medalh√£o com tr√™s camadas principais implementadas em um √∫nico notebook:

1. **Camada Bronze**: Dados brutos ingeridos diretamente dos arquivos .dbc do DATASUS, preservando o formato original com metadados de proveni√™ncia.

2. **Camada Silver**: Dados limpos, validados e enriquecidos com transforma√ß√µes de qualidade e padroniza√ß√£o.

3. **Camada Gold**: Modelo dimensional otimizado para an√°lise com indicadores estrat√©gicos agregados.

### Estrutura do Notebook √önico

O pipeline completo √© executado em sequ√™ncia dentro de um √∫nico notebook:

```python
# SE√á√ÉO 1: CONFIGURA√á√ÉO E IMPORTA√á√ïES
# SE√á√ÉO 2: CAMADA BRONZE - Ingest√£o de dados brutos
# SE√á√ÉO 3: CAMADA SILVER - Transforma√ß√£o e limpeza  
# SE√á√ÉO 4: CAMADA GOLD - Agrega√ß√£o e indicadores
# SE√á√ÉO 5: VALIDA√á√ÉO - Testes e qualidade
```

## ‚öôÔ∏è Pr√©-requisitos e Configura√ß√£o

### Requisitos do Ambiente
- **Databricks Runtime:** 10.4 LTS ou superior
- **Python:** 3.8+
- **PySpark:** 3.2+
- **Bibliotecas:** pyreadstat, delta-spark

### Configura√ß√£o Inicial

1. **Configurar Volume no Databricks:**
```sql
CREATE VOLUME IF NOT EXISTS workspace.default.data
```

2. **Upload dos Arquivos .dbc:**
```bash
# Colocar arquivos no volume criado
# Estrutura esperada:
# /Volumes/workspace/default/data/
#   ‚îú‚îÄ‚îÄ DNSP2010.dbc
#   ‚îú‚îÄ‚îÄ DNSP2011.dbc
#   ‚îú‚îÄ‚îÄ ...
#   ‚îú‚îÄ‚îÄ DOINF2010.dbc
#   ‚îî‚îÄ‚îÄ DOINF2011.dbc
```

## üöÄ Instru√ß√µes de Execu√ß√£o

### Execu√ß√£o do Pipeline Completo

```python
# Executar o notebook completo em sequ√™ncia:
# 1. Configura√ß√£o inicial
# 2. Camada Bronze
# 3. Camada Silver  
# 4. Camada Gold
# 5. Valida√ß√£o final

# Todas as c√©lulas ser√£o executadas sequencialmente
# O tempo total estimado √© de 15-30 minutos dependendo do volume de dados
```

### Execu√ß√£o por Se√ß√µes

**Se√ß√£o 1 - Configura√ß√£o:**
```python
# Configurar ambiente Spark
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")

# Definir caminhos
VOLUME_BASE_PATH = "/Volumes/workspace/default/data/"
```

**Se√ß√£o 2 - Camada Bronze:**
```python
# Executar fun√ß√£o de ingest√£o
ingestao_bronze_completa()
```

**Se√ß√£o 3 - Camada Silver:**
```python
# Processar transforma√ß√µes
transformar_silver_nascimentos()
transformar_silver_obitos()
```

**Se√ß√£o 4 - Camada Gold:**
```python
# Criar modelo dimensional
criar_camada_gold()
```

**Se√ß√£o 5 - Valida√ß√£o:**
```python
# Executar testes
validar_pipeline_completo()
```

## üéØ Decis√µes T√©cnicas e Justificativas

### 1. Notebook √önico com M√∫ltiplas Camadas
**Justificativa:** Implementamos todas as camadas em um √∫nico notebook para:
- **Facilitar a execu√ß√£o** e reprodu√ß√£o do pipeline completo
- **Reduzir depend√™ncias** entre notebooks separados
- **Simplificar a manuten√ß√£o** e versionamento
- **Otimizar o uso de recursos** do Databricks

### 2. Arquitetura Medalh√£o
**Justificativa:** Escolhemos a arquitetura medalh√£o para:
- **Resili√™ncia a mudan√ßas de schema** entre diferentes anos dos dados DATASUS
- **Preserva√ß√£o dos dados originais** na camada Bronze
- **Transforma√ß√£o incremental** com qualidade crescente
- **Reusabilidade** dos dados para m√∫ltiplos prop√≥sitos

### 3. Formato Delta Lake
**Justificativa:** Utilizamos Delta Lake por oferecer:
- **ACID transactions** para garantia de consist√™ncia
- **Schema evolution** nativo para evolu√ß√£o dos dados
- **Time travel** para auditoria e reprocessamento
- **Compaction e otimiza√ß√£o** autom√°tica

### 4. Processamento de Arquivos .dbc
**Justificativa:** Implementamos processamento nativo porque:
- **Evita depend√™ncias externas** (reprodutibilidade)
- **Processamento 100% dentro do Databricks**
- **Controle total** sobre o processo de parsing
- **Adaptabilidade** a mudan√ßas de formato

### 5. Agrega√ß√µes na Camada Gold
**Justificativa:** As agrega√ß√µes mensais foram escolhidas porque:
- **Balanceiam detalhe e performance** para an√°lise
- **Permitem an√°lise temporal** (sazonalidade, tend√™ncias)
- **Facilitam compara√ß√µes** entre per√≠odos e regi√µes
- **Atendem aos requisitos** dos indicadores de sa√∫de solicitados

## ‚úÖ Valida√ß√µes e Testes Realizados

### 1. Valida√ß√£o da Camada Bronze
```python
# Teste de ingest√£o completa
def test_ingestao_bronze():
    sinasc_count = spark.read.table("bronze_sinasc").count()
    sim_count = spark.read.table("bronze_sim").count()
    
    assert sinasc_count > 0, "Bronze SINASC vazia"
    assert sim_count >= 0, "Bronze SIM com problemas"
    
    print(f"‚úÖ Bronze SINASC: {sinasc_count:,} registros")
    print(f"‚úÖ Bronze SIM: {sim_count:,} registros")
```

### 2. Valida√ß√£o da Camada Silver
```python
# Teste de qualidade dos dados Silver
def test_qualidade_silver():
    nascimentos = spark.read.table("silver_nascimentos")
    
    # Testes de integridade
    assert nascimentos.filter(col("data_nascimento").isNull()).count() == 0
    assert nascimentos.filter(col("codigo_municipio_nascimento").isNull()).count() == 0
    
    # Testes de dom√≠nios categ√≥ricos
    sexos_validos = nascimentos.filter(~col("sexo").isin(["Masculino", "Feminino", "Ignorado"])).count()
    assert sexos_validos == 0, "Valores inv√°lidos na coluna sexo"
```

### 3. Valida√ß√£o da Camada Gold
```python
# Teste dos indicadores calculados
def test_indicadores_gold():
    indicadores = spark.read.table("gold_indicadores_saude")
    
    # Verificar que percentuais est√£o entre 0-100
    assert indicadores.filter((col("perc_baixo_peso") < 0) | (col("perc_baixo_peso") > 100)).count() == 0
    
    # Verificar taxas de mortalidade calculadas corretamente
    taxas_corretas = indicadores.filter(
        (col("taxa_mortalidade_infantil") == (col("total_obitos_infantis") / col("total_nascidos_vivos")) * 1000)
    ).count()
    
    assert taxas_corretas == indicadores.count(), "C√°lculo de taxas incorreto"
```

### 4. Valida√ß√£o de Performance
```python
# Teste de performance das consultas
def test_performance():
    import time
    
    start_time = time.time()
    resultado = spark.sql("""
        SELECT sk_tempo, AVG(taxa_mortalidade_infantil) 
        FROM gold_indicadores_saude 
        GROUP BY sk_tempo 
        ORDER BY sk_tempo
    """).count()
    
    tempo_execucao = time.time() - start_time
    assert tempo_execucao < 10, "Consulta muito lenta"
    print(f"‚úÖ Performance: {tempo_execucao:.2f} segundos")
```

### 5. Valida√ß√£o dos Indicadores Obrigat√≥rios
```python
# Verifica√ß√£o de todos os indicadores solicitados
def test_indicadores_obrigatorios():
    indicadores = spark.read.table("gold_indicadores_saude")
    colunas_obrigatorias = [
        "total_nascidos_vivos",
        "perc_prenatal_7_ou_mais_consultas", 
        "perc_baixo_peso",
        "perc_partos_cesarea",
        "perc_maes_adolescentes",
        "total_obitos_infantis",
        "taxa_mortalidade_infantil",
        "total_obitos_neonatais", 
        "taxa_mortalidade_neonatal",
        "total_obitos_maternos",
        "taxa_mortalidade_materna"
    ]
    
    for coluna in colunas_obrigatorias:
        assert coluna in indicadores.columns, f"Coluna {coluna} n√£o encontrada"
    
    print("‚úÖ Todos os indicadores obrigat√≥rios implementados")
```

## üîÆ Pr√≥ximos Passos e Melhorias

### Melhorias T√©cnicas
1. **Implementa√ß√£o de monitoramento** com Databricks Workflows e alertas
2. **Otimiza√ß√£o de performance** com Z-Ordering e Bloom Filters
3. **Data Quality Framework** com valida√ß√µes automatizadas

### Expans√£o do Modelo
1. **Novas fontes de dados:** Incorporar dados do CNES e IBGE
2. **Indicadores avan√ßados:** Anos de vida perdidos, an√°lise de desigualdades
3. **An√°lise preditiva:** Modelos de s√©ries temporais para previs√£o

### Melhorias de Governan√ßa
1. **Cat√°logo de dados** com Unity Catalog
2. **Lineage completo** dos dados
3. **Controle de acesso** granular por camada

---

**Desenvolvido para a Cuidado Conectado** - Transformando dados em insights para sa√∫de p√∫blica.
